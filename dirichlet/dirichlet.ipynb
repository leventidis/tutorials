{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import dirichlet, beta, norm\n",
    "from numpy.random import choice\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large portion of the materials/code in this notebook are adapted or taken from: [TIM HOPPER](https://dp.tdhopper.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet distribution can be considered as a distribution of distributions. Each sample from the Dirichlet distribution is a categorical distribution over $K$ categories. The distribution is parameterized using a $K$ dimensional vector $\\alpha$. This parameter is also known as the concentration parameter. We can express the probability density function of the Dirichlet distribution as follows:\n",
    "\n",
    "$$f(x_1, \\dots, x_K; \\alpha_1, \\dots, \\alpha_K) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^{K}x_{i}^{\\alpha_{i} - 1}$$\n",
    "\n",
    "where $\\{x_{i}\\}_{i=1}^{i=K}$ belong to the $K-1$ [simplex](https://en.wikipedia.org/wiki/Simplex). So $\\sum_{i=1}^{K}x_{i} = 1$ and $x_{i} \\geq 0 \\ \\forall i \\in [1,K]$.\n",
    "\n",
    "Here $B(\\alpha)$ refers to the multivariate [Beta function](https://en.wikipedia.org/wiki/Beta_function) which we can express in terms of the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function) as:\n",
    "$$B(\\alpha) = \\frac{\\prod_{i=1}^{K}\\Gamma(\\alpha_i)}{\\Gamma(\\sum_{i=1}^{K}a_i)}$$\n",
    "\n",
    "The expected value of the Dirichlet distribution depends on the $\\alpha$ vector and is given by:\n",
    "\n",
    "$$\\mathbb{E}[(x_{i}, \\dots, x_{K})] = \\frac{(a_{1}, \\dots, a_{K})}{\\sum_{i=1}^{K}a_{i}}$$\n",
    "\n",
    "As the value (i.e. magnitude) of the concentration parameter $\\alpha$ increases the standard deviation of the distribution decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(scale_factor, G0=[1, 1, 1], N=10000):\n",
    "    samples = dirichlet(alpha = scale_factor * np.array(G0)).rvs(N)\n",
    "    print(\"                          alpha:\", scale_factor)\n",
    "    print(\"              element-wise mean:\", samples.mean(axis=0))\n",
    "    print(\"element-wise standard deviation:\", samples.std(axis=0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we draw 10000 samples from a symmetric Dirichlet distribution (i.e. one where each component of the $\\alpha$ vector is equal).\n",
    "\n",
    "Notice that as we increase the concentration parameter $\\alpha$ the standard deviation of our distribution decreases. The mean value of the distribution remains thesame however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha_val in [0.1, 1, 10, 100, 1000]:\n",
    "    stats(alpha_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dirichlet Process can be seen as an infinite-dimensional generalization of the Dirichlet distribution. More concretely, in the same way as the Dirichlet distribution is the conjugate prior for the categorical distribution, the Dirichlet process is the conjugate prior for the infinite, nonparametric discrete distributions. For example, Dirichlet processes is a prior probability distribution for infinite mixture models.\n",
    "\n",
    "A Dirichlet process is parameterized by a base probability distribution $H$ and a positive real number $\\alpha$ and in general generates samples which are distributions similar to $H$. $H$ is also referred to as a base measure.\n",
    "\n",
    "A sample $G$ (recall that $G$ is also a probability distribution) from a Dirichlet process, $\\textrm{DP}(\\alpha, H)$, can be constructed by drawing a countable infinity of samples $\\theta_{k}$ from $H$. We define $G \\sim \\textrm{DP}(\\alpha, H)$ as:\n",
    "\n",
    "$$G = \\sum_{k=1}^{\\infty} \\pi_{k} \\delta_{\\theta_k}$$\n",
    "\n",
    "$\\pi_k$ are weights that sum to 1 (those weights as we shall later see are also derived from a Dirichlet distribution) and $\\delta$ is the Dirac delta function.\n",
    "\n",
    "Notice that $G$ is a *discrete* distribution that takes the value $\\theta_k$ with probability $\\pi_k$. This is true regardless of if $H$ has a continuous support (a support of a distribution is defined as the set of possible values of a random variable in a distribution). The support of $G$ is a countable infinite subset of the support of $H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Property of the Dirichlet Process\n",
    "[Gregor Heinrich](http://www.arbylon.net/publications/ilda.pdf) writes:\n",
    "\n",
    "<font size=\"5\">\n",
    "The defining property of the DP is that its samples have weights $\\pi_k$ and locations $\\theta_k$ distributed in such a way that when partitioning $S(H)$ into finitely many arbitrary disjoint subsets $S_1, \\dots, S_j$ for $J<\\infty$, the sums of the weights $\\pi_k$ in each of these $J$ subsets are distributed according to a Dirichlet distribution described by $\\mathrm{Dir}(\\alpha G_0)$ where $G_0$ is a discrete base distribution whose weights are equal to the integrals of the base distribution $H$ over the subsets $S_n$.\n",
    "</font>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "For example consider a Dirichlet process with a standard normal base measure $H \\sim \\mathcal{N}(0, 1)$. Let $H$ be a sample from $\\mathrm{DP}(\\alpha, H)$ and partition the real line (the support of a normal distribution) as $S_1=(-\\infty,-1], S_2=(-1,1], and S_3=(1,\\infty]$ then\n",
    "\n",
    "$$H(S_1),H(S_2),H(S_3)\\sim Dir(\\alpha \\ \\mathrm{erf}(-1), \\alpha (\\ \\mathrm{erf}(1) - \\mathrm{erf}(-1)),\\alpha(1 - \\mathrm{erf}(1)))$$\n",
    "\n",
    "Note that $H(S_n)$ denotes the sum of the $\\pi_k$ values whose $\\theta_k$ lie in the partition $S_n$. $\\mathrm{erf}$ refers to the error function which can define integral segments over a Gaussian distribution. This partitioning property holds for any possible partitions over the support of the base measure.\n",
    "\n",
    "**This means that for any sample from a Dirichlet process we can construct a sample from a Dirichlet distribution by partitioning the support of the sample into finite number of bins.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the weights $\\pi$ (Stick Breaking Construction)\n",
    "\n",
    "To satisfy the above property there are various equivalent ways to generate the weights $\\pi_k$ values. Some notable procedures are the Chinese restaurant process, the Polya urn scheme and the Stick breaking construction which we discuss here.\n",
    "\n",
    "Given a stick of length 1 at the beginning of each iteration $k$, we repeatedly break off a fraction $\\beta_{k}$ of the remaining stick. $\\beta_{k} \\in [0,1]$ is sampled from a $Beta(1, \\alpha)$ distribution. The weights $\\pi_{k}$ are defined as the actual length of the stick removed at iteration $k$. So initially $\\beta_1 = \\pi_1$. More generally we have:\n",
    "\n",
    "$$\\pi_k = \\beta_k \\prod_{j=1}^{k-1}(1-\\beta_j)$$\n",
    "\n",
    "So one naive way to draw a sample for a Dirichlet process is to sample an infinite number of $\\theta_k$ values from the base distribution $H$ and infinite number of $\\beta_k$ values from the $Beta(1,\\alpha)$ in order to construct our $\\pi_k$. \n",
    "\n",
    "Notice that the weights $\\pi_k$ are always positive and get progressively smaller as $k\\rightarrow \\infty$. Also notice that $\\sum_{k=1}^{\\infty} \\pi_{k} = 1$. So this means we can approximate $G \\sim \\mathrm{DP}(\\alpha, H)$ with a sufficient amount of samples from $H$ quite accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Dirichlet Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use this stick breaking construction to draw approximate samples from several Dirichlet processes with a standard normal, $\\mathcal{N}(0,1)$, base distribution, $H$, but varying $\\alpha$ parameter.\n",
    "\n",
    "Recall that a single sample from a Dirichlet process is a probability distribution over a countably infinite subset of the support of the base measure.\n",
    "\n",
    "The blue line is the PDF(Probability Density Function) for a standard normal. The black lines represent the $\\theta_k$ and $\\pi_k$ values; $\\theta_k$ is indicated by the position of the black line on the x-axis; $\\pi_k$ is proportional to the height of each line.\n",
    "\n",
    "We generate enough $pi_k$ values are generated so their sum is greater than 0.99. When $\\alpha$ is small, very few $\\theta_k$'s will have corresponding $\\pi_k$ values larger than 0.01. However, as $\\alpha$ grows large, the sample becomes a more accurate (though still discrete) approximation of $\\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_sample_approximation(base_measure, alpha, tol=0.01):\n",
    "    '''\n",
    "    Extract respective pi and theta values for a dirichlet process using the stick breaking construction\n",
    "    This is an approximation such that the pi values sum to more than 1-tol. \n",
    "    '''\n",
    "    betas = []\n",
    "    pis = []\n",
    "    betas.append(beta(1, alpha).rvs())\n",
    "    pis.append(betas[0])\n",
    "    \n",
    "    while sum(pis) < (1.-tol):\n",
    "        s = np.sum([np.log(1 - b) for b in betas])\n",
    "        new_beta = beta(1, alpha).rvs() \n",
    "        betas.append(new_beta)\n",
    "        pis.append(new_beta * np.exp(s))\n",
    "    pis = np.array(pis)\n",
    "    thetas = np.array([base_measure() for _ in pis])\n",
    "    return pis, thetas\n",
    "\n",
    "def plot_normal_dp_approximation(alpha):\n",
    "    fig, ax1 = plt.subplots(figsize=(12,8))\n",
    "    fontsize = 20\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.set_ylabel(\"$\\pi$ value\", fontsize=fontsize)\n",
    "    ax2.set_ylabel(\"Base Measure probability density\", fontsize=fontsize)\n",
    "    fig.suptitle(\"Dirichlet Process Sample with N(0,1) Base Measure \\n $alpha$=\"+str(alpha), fontsize=1.2*fontsize)\n",
    "    \n",
    "    # Sample from base measure and calculate pi values\n",
    "    pis, thetas = dirichlet_sample_approximation(lambda: norm().rvs(), alpha)\n",
    "    ax1.vlines(thetas, 0, pis, )\n",
    "\n",
    "    # Plot the Base measure\n",
    "    X = np.linspace(-4,4,100)\n",
    "    ax2.plot(X, norm.pdf(X))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_normal_dp_approximation(.1)\n",
    "plot_normal_dp_approximation(1)\n",
    "plot_normal_dp_approximation(10)\n",
    "plot_normal_dp_approximation(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to draw samples from a distribution sampled from a *Dirichlet process* instead of from the Dirichlet process itself. Much of the literature on the topic refers to this as sampling from a Dirichlet process.\n",
    "\n",
    "Fortunately, we don't have to draw an infinite number of samples from the base distribution and stick breaking process to do this. Instead, we can draw these samples as they are needed.\n",
    "\n",
    "Suppose, for example, we know a finite number of the $\\theta_k$ and $\\pi_k$ values for a sample $G \\sim \\textrm{Dir}(\\alpha, H)$. For example, we know\n",
    "\n",
    "$$\\pi_1=0.5,\\pi_2=0.3,\\theta_1=0.1,\\theta_2=-0.5.$$\n",
    "\n",
    "To sample from $G$, we can generate a uniform random u number between 0 and 1. If $u$ is less than 0.5, our sample is 0.1. If $0.5\\leq u<0.8$, our sample is $-0.5$. If $u\\geq 0.8$, our sample (from $G$ will be a new sample $\\theta_3$ from $H$. At the same time, we should also sample and store $\\pi_3$. When we draw our next sample, we will again draw $u \\sim \\textrm{Uniform}(0,1)$ but will compare against $\\pi_1,\\pi_2$ AND $pi_3$.\n",
    "\n",
    "The class below will take a base distribution $H$ and $\\alpha$ as arguments to its constructor. The class instance can then be called to generate samples from $G \\sim \\textrm{DP}(\\alpha,  H)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletProcessSample():\n",
    "    '''\n",
    "    Given a base measure and an alpha value return a sample from a distribution\n",
    "    sampled from a Dirichlet process.\n",
    "    '''\n",
    "    def __init__(self, base_measure, alpha):\n",
    "        self.base_measure = base_measure\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.cache = []\n",
    "        self.weights = []\n",
    "        self.total_stick_used = 0.\n",
    "\n",
    "    def __call__(self):\n",
    "        remaining = 1.0 - self.total_stick_used\n",
    "        i = DirichletProcessSample.roll_die(self.weights + [remaining])\n",
    "        if i is not None and i < len(self.weights):\n",
    "            return self.cache[i]\n",
    "        else:\n",
    "            stick_piece = beta(1, self.alpha).rvs() * remaining\n",
    "            self.total_stick_used += stick_piece\n",
    "            self.weights.append(stick_piece)\n",
    "            new_value = self.base_measure()\n",
    "            self.cache.append(new_value)\n",
    "            return new_value\n",
    "        \n",
    "    @staticmethod \n",
    "    def roll_die(weights):\n",
    "        if weights:\n",
    "            return choice(range(len(weights)), p=weights)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_measure = lambda: norm().rvs()\n",
    "n_samples = 20000\n",
    "samples = {}\n",
    "for alpha in [1, 10, 100, 1000]:\n",
    "    dirichlet_norm = DirichletProcessSample(base_measure=base_measure, alpha=alpha)\n",
    "    samples[\"Alpha: %s\" % alpha] = [dirichlet_norm() for _ in range(n_samples)]\n",
    "\n",
    "_ = pd.DataFrame(samples).hist(figsize=(15,15), bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these histograms look very similar to the corresponding plots of sampled distributions above. However, these histograms are plotting points sampled from a distribution sampled from a Dirichlet process while the plots above were showing approximate distributions samples from the Dirichlet process. Of course, as the number of samples from each $G$ grows large, we would expect the histogram to be a very good empirical approximation of $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
